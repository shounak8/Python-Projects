{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for Twitter Sentiment Analysis\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import tweepy\n",
    "import openpyxl\n",
    "from tweepy import OAuthHandler\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''For getting access to Twitter data, Developer Account is required.\n",
    "   Since credentials cannot be revealed, \n",
    "   I have saved the credentials in .csv file and have imported the .csv file as pandas DataFrame'''\n",
    "\n",
    "cred = pd.read_csv('cred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''From pandas DataFrame, I have extracted the credentials needed for accessing the Twitter data'''\n",
    "\n",
    "consumer_key=cred.iloc[0][2]\n",
    "\n",
    "consumer_secret=cred.iloc[1][2]\n",
    "\n",
    "bearer_token=cred.iloc[2][2]\n",
    "\n",
    "access_token=cred.iloc[3][2]\n",
    "\n",
    "access_token_secret=cred.iloc[4][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Twitter API\n",
    "auth = OAuthHandler(consumer_key, consumer_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## api.search will only give first 100 tweets. \n",
    "## Hence, use tweepy.Cursor() which will help us extract as many tweets as we want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the blank Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>User</th>\n",
       "      <th>User_statuses_count</th>\n",
       "      <th>User_followers</th>\n",
       "      <th>User_location</th>\n",
       "      <th>User_verified</th>\n",
       "      <th>rt_count</th>\n",
       "      <th>tweet_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Tweets, User, User_statuses_count, User_followers, User_location, User_verified, rt_count, tweet_date]\n",
       "Index: []"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['Tweets', 'User','User_statuses_count', 'User_followers', 'User_location','User_verified',\n",
    "                           'rt_count','tweet_date'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the functions to get data from Twitter, clean the tweets and analyze the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For getting the data from Twitter and generating dataframe\n",
    "\n",
    "def stream(data,num):\n",
    "    i = 0\n",
    "    for tweet in tweepy.Cursor(api.search, q=data, count = 100, lang='en').items():\n",
    "        print(i, end='\\r')\n",
    "        df.loc[i, 'Tweets'] = tweet.text\n",
    "        df.loc[i, 'User'] = tweet.user.name\n",
    "        df.loc[i, 'User_statuses_count'] = tweet.user.statuses_count\n",
    "        df.loc[i, 'User_followers'] = tweet.user.followers_count\n",
    "        df.loc[i, 'User_location'] = tweet.user.location\n",
    "        df.loc[i, 'User_verified'] = tweet.user.verified\n",
    "        df.loc[i, 'fav_count'] = tweet.favorite_count\n",
    "        df.loc[i, 'rt_count'] = tweet.retweet_count\n",
    "        df.loc[i, 'tweet_date'] = tweet.created_at        \n",
    "        i = i+1\n",
    "        if i == num:\n",
    "            break\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        \n",
    "# Process for cleaning the text data\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "import string\n",
    "pun = string.punctuation.replace('.','')\n",
    "\n",
    "for i in range(0,len(pun)):\n",
    "    stopwords.append(pun[i])\n",
    "stopwords.append('RT')    #as RT is re-tweet in Twitter\n",
    "\n",
    "def remove(arg):\n",
    "    a = wordpunct_tokenize(arg)\n",
    "    b = []\n",
    "    for i in a:\n",
    "        if i not in stopwords:\n",
    "            b.append(i)\n",
    "    return ' '.join(b)        \n",
    "        \n",
    "    \n",
    "# Defining the Subjectivity and Polarity\n",
    "        \n",
    "def subjectivity(arg):\n",
    "    blob = TextBlob(arg)\n",
    "    if blob.subjectivity > 0.66:\n",
    "        return 'high subjectivity'\n",
    "    elif blob.subjectivity > 0.33:\n",
    "        return 'medium subjectivity'\n",
    "    else:\n",
    "        return 'low subjectivity'\n",
    "    \n",
    "def polarity(arg):\n",
    "    blob = TextBlob(arg)\n",
    "    if blob.polarity > 0:\n",
    "        return 'positive sentiment'\n",
    "    elif blob.polarity < 0:\n",
    "        return 'negative sentiment'\n",
    "    else:\n",
    "        return 'neutral sentiment'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aligning the processes for getting the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process():\n",
    "    query = str(input('Please enter what you want to research about: '))\n",
    "    print()\n",
    "    num = int(input('Please enter the total number of tweets you want analyze: '))\n",
    "    print()\n",
    "    stream([query],num)\n",
    "    df['clean'] = df['Tweets'].apply(lambda x: remove(x))\n",
    "    df['Subjectivity'] = df['clean'].apply(lambda x: subjectivity(x))\n",
    "    df['Sentiment'] = df['clean'].apply(lambda x: polarity(x))\n",
    "    df.to_excel('tweet analysis of {}.xlsx'.format(query))\n",
    "    positive = len(df[df['Sentiment'] == 'positive sentiment'])\n",
    "    negative = len(df[df['Sentiment'] == 'negative sentiment'])\n",
    "    neutral = len(df[df['Sentiment'] == 'neutral sentiment'])\n",
    "    print()\n",
    "    return print('Process Completed. \\n\\n Total Positive Sentiments: {}/{}, \\n Total Negative Sentiments: {}/{}, \\n Total Neutral Sentiments : {}/{} '.format(positive,num, negative,num, neutral,num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter what you want to research about: Bengal Elections\n",
      "\n",
      "Please enter the total number of tweets you want analyze: 500\n",
      "\n",
      "499\n",
      "Process Completed. \n",
      "\n",
      " Total Positive Sentiments: 185/500, \n",
      " Total Negative Sentiments: 121/500, \n",
      " Total Neutral Sentiments : 194/500 \n"
     ]
    }
   ],
   "source": [
    "# Example 1\n",
    "process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter what you want to research about: rising covid cases\n",
      "\n",
      "Please enter the total number of tweets you want analyze: 300\n",
      "\n",
      "299\n",
      "Process Completed. \n",
      "\n",
      " Total Positive Sentiments: 183/300, \n",
      " Total Negative Sentiments: 104/300, \n",
      " Total Neutral Sentiments : 213/300 \n"
     ]
    }
   ],
   "source": [
    "# Example 2\n",
    "process()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
